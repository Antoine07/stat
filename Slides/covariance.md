---
marp: true
theme: default
paginate: true
class: lead
---

# Covariance et ANOVA

**Comprendre le lien entre variables et comparer des groupes**

---

## Pourquoi ce chapitre ?

Jusqu'ici, vous savez :

- d√©crire une variable (moyenne, dispersion),
- visualiser des distributions.

On va maintenant r√©pondre √† deux nouvelles questions :

1. **Deux variables √©voluent-elles ensemble ?** ‚Üí *Covariance*
2. **Plusieurs groupes ont-ils la m√™me moyenne ?** ‚Üí *ANOVA*

---

# PARTIE 1 ‚Äî COVARIANCE

---

## Probl√®me pos√©

On observe deux variables sur les m√™mes individus :

- taille et poids
- temps d'entra√Ænement et performance
- √¢ge et salaire

üëâ **Quand l'une augmente, que fait l'autre ?**

---

## Exemple simple

```python
import numpy as np

x = np.array([1, 2, 3, 4, 5])   # heures de travail
y = np.array([10, 12, 15, 18, 20])  # score
```

---

## Visualisation indispensable

```python
import matplotlib.pyplot as plt

plt.scatter(x, y)
plt.xlabel("x")
plt.ylabel("y")
plt.show()
```

---

## Intuition de la covariance

> La covariance mesure **si deux variables varient ensemble**.

- si x ‚Üë et y ‚Üë ‚Üí covariance **positive**
- si x ‚Üë et y ‚Üì ‚Üí covariance **n√©gative**
- si pas de lien clair ‚Üí covariance ‚âà 0

---

## D√©finition math√©matique

$$
\text{Cov}(X,Y)
= \frac{1}{n} \sum (x_i - \bar x)(y_i - \bar y)
$$

üëâ On compare les **√©carts √† la moyenne**.

---

## Lecture intuitive du produit

- (x ‚àí moyenne) et (y ‚àí moyenne) **m√™me signe** ‚Üí produit positif
- signes oppos√©s ‚Üí produit n√©gatif

Donc :

- majorit√© de produits positifs ‚Üí covariance positive
- majorit√© n√©gatifs ‚Üí covariance n√©gative

---

## Calcul en Python

```python
np.cov(x, y)
```

La covariance recherch√©e est :

```python
np.cov(x, y)[0, 1]
```

---

## Probl√®me majeur de la covariance ‚ö†Ô∏è

> **La covariance d√©pend des unit√©s.**

Exemple :

- cm √ó kg
- heures √ó points

üëâ Impossible de comparer directement deux covariances.

C'est pour cela que l'on introduira ensuite la **corr√©lation**.

---

## Exercice 1 ‚Äî Covariance

```python
a = np.array([2, 4, 6, 8])
b = np.array([8, 6, 4, 2])
```

1. Tracez le nuage de points
2. Calculez la covariance
3. Interpr√©tez le signe

```python
# TODO
```

---

## R√©sum√© Covariance

1. Mesure la variation conjointe de deux variables
2. Signe ‚Üí sens de la relation
3. Valeur brute ‚Üí d√©pend des unit√©s
4. Outil fondamental avant corr√©lation et ACP

---

# PARTIE 2 ‚Äî ANOVA (Analyse de la variance)

---

## Probl√®me pos√©

On a **plusieurs groupes** et on veut savoir :

> **Ont-ils la m√™me moyenne ou pas ?**

Exemples :

- m√©thodes p√©dagogiques
- traitements m√©dicaux
- machines
- groupes d'individus

---

## Exemple concret

```python
A = np.array([12, 13, 11, 12])
B = np.array([16, 17, 15, 16])
C = np.array([10, 9, 11, 10])
```

---

## Moyennes par groupe

```python
A.mean(), B.mean(), C.mean()
```

On observe des diff√©rences‚Ä¶
üëâ mais sont-elles **r√©elles** ou **dues au hasard** ?

---

## Pourquoi le hasard peut tromper

M√™me si les **vraies moyennes** sont √©gales :

- les donn√©es sont dispers√©es,
- les moyennes observ√©es fluctuent.

üëâ L'ANOVA sert √† d√©cider **objectivement**.

---

## Principe fondamental de l'ANOVA

L'ANOVA compare :

1. la **variabilit√© entre les groupes**
2. la **variabilit√© √† l'int√©rieur des groupes**

---

## Intuition cl√©

> Si les groupes sont tr√®s diff√©rents **entre eux**,
> mais homog√®nes **en leur sein**,
> alors les moyennes sont probablement diff√©rentes.

---

## Sch√©ma conceptuel

![Image](https://www.researchgate.net/publication/329788831/figure/fig2/AS%3A711397558206464%401546621803288/Graphical-representation-of-the-rationale-behind-the-analysis-of-variance-ANOVA-A.png)

---

![Image](https://stpltrsrcscmnprdwus001.blob.core.windows.net/rsrcs/srm/images/research-methods-statistics-for-public-nonprofit-administrators-practical-guide/10.4135_9781544307763-fig10-1.jpg)

- dispersion interne = bruit
- dispersion entre groupes = signal

---

## Hypoth√®ses statistiques

- **H‚ÇÄ** : toutes les moyennes sont √©gales
- **H‚ÇÅ** : au moins une moyenne est diff√©rente

---

## Statistique F

$$
F = \frac{\text{Variance entre groupes}}
{\text{Variance intra-groupes}}
$$

- F ‚âà 1 ‚Üí pas de diff√©rence claire
- F grand ‚Üí diff√©rence probable

---

## ANOVA en Python (SciPy)

```python
from scipy.stats import f_oneway

f_stat, p_value = f_oneway(A, B, C)
f_stat, p_value
```

---

## Interpr√©tation de la p-value

- p < 0.05 ‚Üí diff√©rence significative
- p ‚â• 0.05 ‚Üí pas de conclusion claire

üëâ **ANOVA ‚â† dire quels groupes diff√®rent**,
mais **dire qu'il existe une diff√©rence**.

---

## Exercice 2 ‚Äî ANOVA

```python
G1 = np.array([10, 11, 10, 9])
G2 = np.array([12, 13, 14, 13])
G3 = np.array([10, 9, 8, 9])
```

1. Calculez les moyennes
2. Lancez l'ANOVA
3. Concluez

```python
# TODO
```

---

## Conditions d'application (simplifi√©es)

ANOVA est pertinente si :

1. donn√©es quantitatives
2. groupes ind√©pendants
3. distributions √† peu pr√®s normales
4. dispersions comparables

---

## Lien avec ce que vous connaissez d√©j√†

- variance ‚Üí dispersion
- √©cart-type ‚Üí homog√©n√©it√©
- centrage-r√©duction ‚Üí comparaison √©quitable
- ANOVA ‚Üí g√©n√©ralisation du test de moyenne

---

## R√©sum√© g√©n√©ral

### Covariance

- mesure la variation conjointe
- d√©pend des unit√©s
- base de la corr√©lation

### ANOVA

- compare plusieurs moyennes
- s'appuie sur la variance
- distingue hasard / effet r√©el

Parfait. Voici la **suite naturelle du cours**, toujours **d√©butant Bac S**, structur√©e **comme vos slides**, avec **intuition ‚Üí maths ‚Üí Python ‚Üí exercices**, couvrant :

1. **Corr√©lation = covariance normalis√©e**
2. **Lien covariance ‚Üî matrice**
3. **ACP avec matrices, valeurs propres et vecteurs propres (sans lourdeur inutile)**

---

# Corr√©lation et ACP

**Du lien entre variables √† la r√©duction de dimension**

---

# PARTIE 1 ‚Äî CORR√âLATION

*(normalisation de la covariance)*

---

## Probl√®me de la covariance (rappel)

La covariance indique :

- le **sens** du lien,
- mais d√©pend des **unit√©s**.

üëâ Impossible de dire si une covariance est ‚Äúforte‚Äù ou ‚Äúfaible‚Äù.

---

## Id√©e cl√© de la corr√©lation

> **La corr√©lation est une covariance rendue sans unit√©.**

On divise la covariance par les dispersions des deux variables.

---

## Formule de la corr√©lation de Pearson

$$
r = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}
$$

o√π :

- (\sigma_X) = √©cart-type de X
- (\sigma_Y) = √©cart-type de Y

---

## Propri√©t√©s fondamentales

- ( -1 \le r \le 1 )
- r > 0 ‚Üí lien croissant
- r < 0 ‚Üí lien d√©croissant
- r ‚âà 0 ‚Üí pas de lien lin√©aire

---

## Intuition g√©om√©trique (tr√®s importante)

> La corr√©lation mesure **l'alignement** des points.

<img src="https://qi.elft.nhs.uk/wp-content/uploads/2014/08/scatter-plot-31.png" width="800" />

---



<img src="https://www.itl.nist.gov/div898/handbook/eda/section3/gif/scatplo3.gif" width="800" />

---

- nuage align√© ‚Üí |r| proche de 1
- nuage circulaire ‚Üí r proche de 0

---

## Corr√©lation = covariance de donn√©es centr√©es-r√©duites

Si X et Y sont **centr√©es-r√©duites** :

$$
\text{Cov}(X,Y) = r
$$

üëâ C'est pour cela que la **corr√©lation est centrale en ACP**.

---

## Calcul en Python

```python
import numpy as np

x = np.array([1, 2, 3, 4, 5])
y = np.array([10, 12, 15, 18, 20])

np.corrcoef(x, y)[0, 1]
```

---

## Exercice 1 ‚Äî Corr√©lation

```python
a = np.array([1, 2, 3, 4, 5])
b = np.array([5, 4, 3, 2, 1])
```

1. Calculez la corr√©lation
2. Interpr√©tez le signe et la valeur

```python
# TODO
```

---

## Corr√©lation ‚â† causalit√© (rappel essentiel)

- forte corr√©lation ‚â† cause
- corr√©lation mesure **co-variation**, pas m√©canisme

üëâ Toujours regarder le **contexte** et le **nuage de points**.

---

# PARTIE 2 ‚Äî MATRICES & COVARIANCE MULTIVARI√âE

---

## Plusieurs variables = matrice de donn√©es

$$
X =
\begin{pmatrix}
x_{11} & x_{12} & \dots \
x_{21} & x_{22} & \dots \
\vdots & \vdots & \ddots
\end{pmatrix}
$$

- lignes ‚Üí individus
-  colonnes ‚Üí variables

---

## Matrice de covariance

> G√©n√©ralisation de la variance et de la covariance.

```python
Xc = X - X.mean(axis=0)
Cov = np.cov(Xc, rowvar=False)
```

---

## Lecture de la matrice de covariance

- diagonale ‚Üí variances
- hors diagonale ‚Üí covariances
- sym√©trique

---

## Version normalis√©e : matrice de corr√©lation

```python
Corr = np.corrcoef(X, rowvar=False)
```

- diagonale = 1
- valeurs entre ‚àí1 et 1

üëâ **Base de l'ACP normalis√©e**

---

# PARTIE 3 ‚Äî ACP (Analyse en Composantes Principales)

---

## Probl√®me pos√© par l'ACP

- beaucoup de variables
- variables corr√©l√©es
- difficile √† visualiser

üëâ **r√©sumer l'information sans trop la perdre**

---

## Id√©e g√©om√©trique centrale

> L'ACP cherche **les directions o√π les donn√©es varient le plus**.

<img src="https://www.researchgate.net/publication/320201024/figure/fig1/AS%3A668939172474885%401536498935431/D-scatter-plot-of-principal-component-analysis-PCA-projected-i-vectors-for-different.png" width="700" />


---


![Image](https://miro.medium.com/1%2A_wcd4AGrcovM0m_WypIYtQ.png)

---

## √âtape 1 ‚Äî Centrage (obligatoire)

```python
Xc = X - X.mean(axis=0)
```

---

## √âtape 2 ‚Äî (souvent) R√©duction

```python
Xs = Xc / X.std(axis=0)
```

Utilis√©e quand :

- unit√©s diff√©rentes
- √©chelles incomparables

---

## √âtape 3 ‚Äî Matrice de covariance

```python
Cov = np.cov(Xs, rowvar=False)
```

---

## √âtape 4 ‚Äî Valeurs propres et vecteurs propres

```python
vals, vecs = np.linalg.eig(Cov)
```

### Interpr√©tation

- **vecteurs propres** ‚Üí directions principales
- ***valeurs propres** ‚Üí quantit√© de variance expliqu√©e

---

## Sens g√©om√©trique

- vecteur propre = axe de projection
- valeur propre = importance de cet axe

üëâ PC1 = direction la plus ‚Äúlongue‚Äù du nuage

---

## √âtape 5 ‚Äî Tri par importance

```python
idx = np.argsort(vals)[::-1]
vals = vals[idx]
vecs = vecs[:, idx]
```

---

## √âtape 6 ‚Äî Projection des donn√©es

```python
X_proj = Xs @ vecs
```

- nouvelles coordonn√©es
- m√™mes individus
- nouvelles variables : PC1, PC2, ‚Ä¶

---

## Exercice 2 ‚Äî ACP minimale

```python
X = np.array([
    [170, 65],
    [180, 80],
    [160, 55],
    [175, 75]
], dtype=float)
```

1. Centrez les donn√©es
2. Calculez la covariance
3. Trouvez PC1
4. Projetez les donn√©es

```python
# TODO
```

---

## Combien de composantes garder ?

Variance expliqu√©e :

```python
vals / vals.sum()
```

- PC1 + PC2 ‚âà 80‚Äì90 % ‚Üí souvent suffisant

---

## R√©sum√© ACP 

1. L'ACP travaille sur les **corr√©lations**
2. Elle cherche des **axes optimaux**
3. Elle remplace plusieurs variables par quelques-unes
4. Elle repose sur :

   - matrices
   - valeurs propres
   - vecteurs propres

---

## R√©sum√© global du chapitre

### Corr√©lation

- covariance normalis√©e
- sans unit√©
- base de l'analyse multivari√©e

---

### ACP

- rotation du nuage
- r√©duction de dimension
- outil central en data science


---

# Cercle de corr√©lation

**Interpr√©tation graphique de l'ACP**

---

## Pourquoi le cercle de corr√©lation ?

Apr√®s l'ACP, on a :

- des **individus projet√©s** (PC1, PC2),
- mais aussi des **variables d'origine**.

üëâ Le cercle de corr√©lation sert √† r√©pondre √† ces questions :

1. Quelles variables expliquent **PC1** ? **PC2** ?
2. Quelles variables sont **corr√©l√©es entre elles** ?
3. Quelles variables sont **ind√©pendantes** ?
4. Quelles variables sont **bien ou mal repr√©sent√©es** ?

---

## Id√©e cl√© (√† retenir absolument)

> Le cercle de corr√©lation repr√©sente **les corr√©lations entre les variables d'origine et les axes principaux (PC1, PC2)**.

Ce ne sont **pas les individus**,
ce sont **les variables**.

---

## Pourquoi un cercle de rayon 1 ?

Les donn√©es sont **centr√©es-r√©duites** :

- variance = 1
- corr√©lations ‚àà [‚àí1, +1]

üëâ Les coordonn√©es des variables sont des **corr√©lations**
üëâ Elles sont donc **born√©es par 1**

---

## Sch√©ma conceptuel


<img src="https://www.researchgate.net/publication/328464392/figure/fig5/AS%3A941885082329088%401601574311006/PCA-correlation-circle-variables-points-and-samples-circles-Percent-on-each.png" width="500" />

---

![Image](https://i.sstatic.net/AdtVP.png)

- chaque fl√®che = une variable
- longueur = qualit√© de repr√©sentation
- angle = corr√©lation entre variables

---

## Construction math√©matique (simple)

Apr√®s l'ACP :

- `vecs` = vecteurs propres
- `vals` = valeurs propres

Les coordonn√©es des variables dans le cercle :

$$
\text{corr}(X_j, PC_k) = \sqrt{\lambda_k} \cdot v_{jk}
$$

En Python :

```python
corr = vecs * np.sqrt(vals)
```

---

## Coordonn√©es pour PC1‚ÄìPC2

```python
corr_circle = corr[:, :2]
```

Chaque ligne :

- une variable
- colonne 1 ‚Üí PC1
- colonne 2 ‚Üí PC2

---

## Exemple concret

Variables :

- Vitesse
- Force
- Endurance
- Agilit√©

---

Coordonn√©es possibles :

| Variable  | PC1  | PC2   |
| --------- | ---- | ----- |
| Vitesse   | 0.85 | 0.10  |
| Force     | 0.88 | 0.05  |
| Endurance | 0.82 | 0.20  |
| Agilit√©   | 0.60 | ‚àí0.60 |

---

## Lecture fondamentale ‚Äî longueur de la fl√®che

> Plus une fl√®che est proche du cercle unit√©,
> mieux la variable est expliqu√©e par PC1 et PC2.

- proche du cercle ‚Üí information bien r√©sum√©e
- proche du centre ‚Üí information mal capt√©e

---

## Lecture fondamentale ‚Äî angle entre deux fl√®ches

> L'angle entre deux variables ‚âà corr√©lation entre elles

- angle ‚âà 0¬∞ ‚Üí corr√©lation forte positive
- angle ‚âà 180¬∞ ‚Üí corr√©lation forte n√©gative
- angle ‚âà 90¬∞ ‚Üí variables non corr√©l√©es

---

## Interpr√©tation visuelle

![Image](https://i.sstatic.net/2pjd8.png)

---

![Image](https://www.researchgate.net/publication/362246632/figure/fig2/AS%3A11431281275524642%401725413786376/Graphical-results-from-the-Principal-Component-Analysis-PCA-a-Variables-factor-map.tif)

---

## Exemple d'interpr√©tation (typique)

1. Vitesse, Force, Endurance :

   - fl√®ches longues
   - m√™me direction
     ‚Üí **fortement corr√©l√©es**
     ‚Üí expliquent principalement PC1

2. Agilit√© :

   - orientation diff√©rente
   - composante PC2 marqu√©e
     ‚Üí information compl√©mentaire

---

## Code complet de trac√©

```python
import numpy as np
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(6, 6))

theta = np.linspace(0, 2*np.pi, 300)
ax.plot(np.cos(theta), np.sin(theta))

ax.axhline(0)
ax.axvline(0)

for i, var in enumerate(variables):
    x, y = corr_circle[i]
    ax.arrow(0, 0, x, y, head_width=0.03, length_includes_head=True)
    ax.text(x*1.1, y*1.1, var)

ax.set_aspect("equal")
ax.set_xlabel("PC1")
ax.set_ylabel("PC2")
ax.set_title("Cercle de corr√©lation")
plt.show()
```

---

## Erreurs fr√©quentes √† √©viter

‚ùå Confondre individus et variables
‚ùå Interpr√©ter une fl√®che courte
‚ùå Oublier le centrage-r√©duction
‚ùå Interpr√©ter PC3 sur un cercle PC1‚ÄìPC2

---

## Exercice 1 ‚Äî Lecture guid√©e

On observe un cercle o√π :

- A et B sont align√©es
- C est orthogonale √† A
- D est proche du centre

Questions :

1. Quelles variables sont corr√©l√©es ?
2. Quelle variable est mal expliqu√©e ?
3. Quelle variable explique PC2 ?

---

## Exercice 2 ‚Äî Analyse compl√®te

√Ä partir du cercle de corr√©lation que vous avez obtenu :

1. Identifiez les groupes de variables corr√©l√©es
2. Donnez une interpr√©tation de PC1
3. Donnez une interpr√©tation de PC2
4. Proposez un nom √† chaque axe

---

## R√©sum√©

1. Le cercle repr√©sente les **variables**
2. Les coordonn√©es sont des **corr√©lations**
3. Longueur = qualit√© de repr√©sentation
4. Angle = corr√©lation entre variables
5. PC1 et PC2 r√©sument l'essentiel
